"use strict";(self.webpackChunkflowtorch=self.webpackChunkflowtorch||[]).push([[1756],{2596:(e,a,t)=>{t.r(a),t.d(a,{contentTitle:()=>c,default:()=>b,frontMatter:()=>m,metadata:()=>h,toc:()=>u});var n=t(7462),s=(t(7294),t(3905)),o=t(2814),l=t(1436),r=t(1032),d=(t(8666),t(2520)),i=(t(84),t(7868));const m={id:"flowtorch.bijectors.splineautoregressive",sidebar_label:"SplineAutoregressive"},c=void 0,h={unversionedId:"api/flowtorch.bijectors.splineautoregressive",id:"api/flowtorch.bijectors.splineautoregressive",isDocsHomePage:!1,title:"flowtorch.bijectors.splineautoregressive",description:"flowtorch  bijectors  SplineAutoregressive",source:"@site/docs/api/flowtorch.bijectors.splineautoregressive.mdx",sourceDirName:"api",slug:"/api/flowtorch.bijectors.splineautoregressive",permalink:"/api/flowtorch.bijectors.splineautoregressive",editUrl:"https://github.com/facebookincubator/flowtorch/edit/main/website/docs/api/flowtorch.bijectors.splineautoregressive.mdx",tags:[],version:"current",frontMatter:{id:"flowtorch.bijectors.splineautoregressive",sidebar_label:"SplineAutoregressive"},sidebar:"apiSidebar",previous:{title:"Spline",permalink:"/api/flowtorch.bijectors.spline"},next:{title:"Tanh",permalink:"/api/flowtorch.bijectors.tanh"}},u=[{value:'<span className="doc-symbol-name">flowtorch.bijectors.SplineAutoregressive</span>',id:"class",children:[{value:'<span className="doc-symbol-name">__init__</span>',id:"--init--",children:[],level:3},{value:'<span className="doc-symbol-name">add_module</span>',id:"add-module",children:[],level:3},{value:'<span className="doc-symbol-name">apply</span>',id:"apply",children:[],level:3},{value:'<span className="doc-symbol-name">bfloat16</span>',id:"bfloat16",children:[],level:3},{value:'<span className="doc-symbol-name">buffers</span>',id:"buffers",children:[],level:3},{value:'<span className="doc-symbol-name">children</span>',id:"children",children:[],level:3},{value:'<span className="doc-symbol-name">cpu</span>',id:"cpu",children:[],level:3},{value:'<span className="doc-symbol-name">cuda</span>',id:"cuda",children:[],level:3},{value:'<span className="doc-symbol-name">double</span>',id:"double",children:[],level:3},{value:'<span className="doc-symbol-name">eval</span>',id:"eval",children:[],level:3},{value:'<span className="doc-symbol-name">extra_repr</span>',id:"extra-repr",children:[],level:3},{value:'<span className="doc-symbol-name">float</span>',id:"float",children:[],level:3},{value:'<span className="doc-symbol-name">forward</span>',id:"forward",children:[],level:3},{value:'<span className="doc-symbol-name">forward_shape</span>',id:"forward-shape",children:[],level:3},{value:'<span className="doc-symbol-name">get_buffer</span>',id:"get-buffer",children:[],level:3},{value:'<span className="doc-symbol-name">get_extra_state</span>',id:"get-extra-state",children:[],level:3},{value:'<span className="doc-symbol-name">get_parameter</span>',id:"get-parameter",children:[],level:3},{value:'<span className="doc-symbol-name">get_submodule</span>',id:"get-submodule",children:[],level:3},{value:'<span className="doc-symbol-name">half</span>',id:"half",children:[],level:3},{value:'<span className="doc-symbol-name">inverse</span>',id:"inverse",children:[],level:3},{value:'<span className="doc-symbol-name">inverse_shape</span>',id:"inverse-shape",children:[],level:3},{value:'<span className="doc-symbol-name">ipu</span>',id:"ipu",children:[],level:3},{value:'<span className="doc-symbol-name">load_state_dict</span>',id:"load-state-dict",children:[],level:3},{value:'<span className="doc-symbol-name">log_abs_det_jacobian</span>',id:"log-abs-det-jacobian",children:[],level:3},{value:'<span className="doc-symbol-name">modules</span>',id:"modules",children:[],level:3},{value:'<span className="doc-symbol-name">named_buffers</span>',id:"named-buffers",children:[],level:3},{value:'<span className="doc-symbol-name">named_children</span>',id:"named-children",children:[],level:3},{value:'<span className="doc-symbol-name">named_modules</span>',id:"named-modules",children:[],level:3},{value:'<span className="doc-symbol-name">named_parameters</span>',id:"named-parameters",children:[],level:3},{value:'<span className="doc-symbol-name">param_shapes</span>',id:"param-shapes",children:[],level:3},{value:'<span className="doc-symbol-name">parameters</span>',id:"parameters",children:[],level:3},{value:'<span className="doc-symbol-name">register_backward_hook</span>',id:"register-backward-hook",children:[],level:3},{value:'<span className="doc-symbol-name">register_buffer</span>',id:"register-buffer",children:[],level:3},{value:'<span className="doc-symbol-name">register_forward_hook</span>',id:"register-forward-hook",children:[],level:3},{value:'<span className="doc-symbol-name">register_forward_pre_hook</span>',id:"register-forward-pre-hook",children:[],level:3},{value:'<span className="doc-symbol-name">register_full_backward_hook</span>',id:"register-full-backward-hook",children:[],level:3},{value:'<span className="doc-symbol-name">register_load_state_dict_post_hook</span>',id:"register-load-state-dict-post-hook",children:[],level:3},{value:'<span className="doc-symbol-name">register_module</span>',id:"register-module",children:[],level:3},{value:'<span className="doc-symbol-name">register_parameter</span>',id:"register-parameter",children:[],level:3},{value:'<span className="doc-symbol-name">requires_grad_</span>',id:"requires-grad-",children:[],level:3},{value:'<span className="doc-symbol-name">set_extra_state</span>',id:"set-extra-state",children:[],level:3},{value:'<span className="doc-symbol-name">share_memory</span>',id:"share-memory",children:[],level:3},{value:'<span className="doc-symbol-name">state_dict</span>',id:"state-dict",children:[],level:3},{value:'<span className="doc-symbol-name">to</span>',id:"to",children:[],level:3},{value:'<span className="doc-symbol-name">to_empty</span>',id:"to-empty",children:[],level:3},{value:'<span className="doc-symbol-name">train</span>',id:"train",children:[],level:3},{value:'<span className="doc-symbol-name">type</span>',id:"type",children:[],level:3},{value:'<span className="doc-symbol-name">xpu</span>',id:"xpu",children:[],level:3},{value:'<span className="doc-symbol-name">zero_grad</span>',id:"zero-grad",children:[],level:3}],level:2}],p={toc:u};function b(e){let{components:a,...t}=e;return(0,s.kt)("wrapper",(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,s.kt)(i.Z,{url:"https://github.com/facebookincubator/flowtorch/blob/main/flowtorch/bijectors/spline_autoregressive.py",mdxType:"PythonNavbar"},(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"/api/flowtorch"},"flowtorch")," ",(0,s.kt)(o.G,{icon:l.cLY,size:"sm",mdxType:"FontAwesomeIcon"})," ",(0,s.kt)("a",{parentName:"p",href:"/api/flowtorch.bijectors"},"bijectors")," ",(0,s.kt)(o.G,{icon:l.cLY,size:"sm",mdxType:"FontAwesomeIcon"})," ",(0,s.kt)("em",{parentName:"p"},"SplineAutoregressive"))),(0,s.kt)(r.Z,{mdxType:"PythonClass"},(0,s.kt)("div",{className:"doc-class-row"},(0,s.kt)("div",{className:"doc-class-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"class")),(0,s.kt)("div",{className:"doc-class-signature"},(0,s.kt)("h2",{id:"class"},(0,s.kt)("span",{className:"doc-symbol-name"},"flowtorch.bijectors.SplineAutoregressive")),(0,s.kt)("span",{className:"doc-inherits-from"},"Inherits from: ",(0,s.kt)("span",{className:"doc-symbol-name"},"flowtorch.bijectors.ops.spline.Spline, flowtorch.bijectors.autoregressive.Autoregressive"))))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"empty docstring\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"--init--"},(0,s.kt)("span",{className:"doc-symbol-name"},"_","_","init","_","_")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, params_fn: Union[flowtorch.lazy.Lazy, NoneType] = None, *, shape: torch.Size, context_shape: Union[torch.Size, NoneType] = None, count_bins: int = 8, bound: float = 3.0, order: str = 'linear') -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"<empty docstring>\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"add-module"},(0,s.kt)("span",{className:"doc-symbol-name"},"add","_","module")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Adds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nArgs:\nname (str): name of the child module. The child module can be\naccessed from this module using the given name\nmodule (Module): child module to be added to the module.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"apply"},(0,s.kt)("span",{className:"doc-symbol-name"},"apply")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\nas well as self. Typical use includes initializing the parameters of a model\n(see also :ref:`nn-init-doc`).\n\nArgs:\nfn (:class:`Module` -> None): function to be applied to each submodule\n\nReturns:\nModule: self\n\nExample::\n\n>>> @torch.no_grad()\n>>> def init_weights(m):\n>>>     print(m)\n>>>     if type(m) == nn.Linear:\n>>>         m.weight.fill_(1.0)\n>>>         print(m.weight)\n>>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n>>> net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[1., 1.],\n[1., 1.]], requires_grad=True)\nSequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"bfloat16"},(0,s.kt)("span",{className:"doc-symbol-name"},"bfloat16")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"buffers"},(0,s.kt)("span",{className:"doc-symbol-name"},"buffers")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, recurse: bool = True) -> Iterator[torch.Tensor]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over module buffers.\n\nArgs:\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\ntorch.Tensor: module buffer\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for buf in model.buffers():\n>>>     print(type(buf), buf.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"children"},(0,s.kt)("span",{className:"doc-symbol-name"},"children")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self) -> Iterator[ForwardRef('Module')]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over immediate children modules.\n\nYields:\nModule: a child module\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"cpu"},(0,s.kt)("span",{className:"doc-symbol-name"},"cpu")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Moves all model parameters and buffers to the CPU.\n\n.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"cuda"},(0,s.kt)("span",{className:"doc-symbol-name"},"cuda")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Moves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on GPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"double"},(0,s.kt)("span",{className:"doc-symbol-name"},"double")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"eval"},(0,s.kt)("span",{className:"doc-symbol-name"},"eval")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Sets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nThis is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.eval()` and several similar mechanisms that may be confused with it.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"extra-repr"},(0,s.kt)("span",{className:"doc-symbol-name"},"extra","_","repr")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self) -> str")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Set the extra representation of the module\n\nTo print customized extra information, you should re-implement\nthis method in your own modules. Both single-line and multi-line\nstrings are acceptable.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"float"},(0,s.kt)("span",{className:"doc-symbol-name"},"float")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"forward"},(0,s.kt)("span",{className:"doc-symbol-name"},"forward")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, x: 'torch.Tensor', context: 'Optional[torch.Tensor]' = None) -> 'torch.Tensor'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"<empty docstring>\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"forward-shape"},(0,s.kt)("span",{className:"doc-symbol-name"},"forward","_","shape")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, shape: 'torch.Size') -> 'torch.Size'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nInfers the shape of the forward computation, given the input shape.\nDefaults to preserving shape.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"get-buffer"},(0,s.kt)("span",{className:"doc-symbol-name"},"get","_","buffer")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, target: str) -> 'Tensor'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nReturns the buffer given by ``target`` if it exists,\notherwise throws an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the buffer\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.Tensor: The buffer referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not a\nbuffer\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"get-extra-state"},(0,s.kt)("span",{className:"doc-symbol-name"},"get","_","extra","_","state")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self) -> Any")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nReturns any extra state to include in the module's state_dict.\nImplement this and a corresponding :func:`set_extra_state` for your module\nif you need to store extra state. This function is called when building the\nmodule's `state_dict()`.\n\nNote that extra state should be pickleable to ensure working serialization\nof the state_dict. We only provide provide backwards compatibility guarantees\nfor serializing Tensors; other objects may break backwards compatibility if\ntheir serialized pickled form changes.\n\nReturns:\nobject: Any extra state to store in the module's state_dict\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"get-parameter"},(0,s.kt)("span",{className:"doc-symbol-name"},"get","_","parameter")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, target: str) -> 'Parameter'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nReturns the parameter given by ``target`` if it exists,\notherwise throws an error.\n\nSee the docstring for ``get_submodule`` for a more detailed\nexplanation of this method's functionality as well as how to\ncorrectly specify ``target``.\n\nArgs:\ntarget: The fully-qualified string name of the Parameter\nto look for. (See ``get_submodule`` for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Parameter: The Parameter referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Parameter``\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"get-submodule"},(0,s.kt)("span",{className:"doc-symbol-name"},"get","_","submodule")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, target: str) -> 'Module'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'\nReturns the submodule given by ``target`` if it exists,\notherwise throws an error.\n\nFor example, let\'s say you have an ``nn.Module`` ``A`` that\nlooks like this:\n\n.. code-block:: text\n\nA(\n(net_b): Module(\n(net_c): Module(\n(conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n)\n(linear): Linear(in_features=100, out_features=200, bias=True)\n)\n)\n\n(The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\nsubmodule ``net_b``, which itself has two submodules ``net_c``\nand ``linear``. ``net_c`` then has a submodule ``conv``.)\n\nTo check whether or not we have the ``linear`` submodule, we\nwould call ``get_submodule("net_b.linear")``. To check whether\nwe have the ``conv`` submodule, we would call\n``get_submodule("net_b.net_c.conv")``.\n\nThe runtime of ``get_submodule`` is bounded by the degree\nof module nesting in ``target``. A query against\n``named_modules`` achieves the same result, but it is O(N) in\nthe number of transitive modules. So, for a simple check to see\nif some submodule exists, ``get_submodule`` should always be\nused.\n\nArgs:\ntarget: The fully-qualified string name of the submodule\nto look for. (See above example for how to specify a\nfully-qualified string.)\n\nReturns:\ntorch.nn.Module: The submodule referenced by ``target``\n\nRaises:\nAttributeError: If the target string references an invalid\npath or resolves to something that is not an\n``nn.Module``\n\n')),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"half"},(0,s.kt)("span",{className:"doc-symbol-name"},"half")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\nThis method modifies the module in-place.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"inverse"},(0,s.kt)("span",{className:"doc-symbol-name"},"inverse")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, y: torch.Tensor, x: Union[torch.Tensor, NoneType] = None, context: Union[torch.Tensor, NoneType] = None) -> torch.Tensor")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"<empty docstring>\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"inverse-shape"},(0,s.kt)("span",{className:"doc-symbol-name"},"inverse","_","shape")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, shape: 'torch.Size') -> 'torch.Size'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nInfers the shapes of the inverse computation, given the output shape.\nDefaults to preserving shape.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"ipu"},(0,s.kt)("span",{className:"doc-symbol-name"},"ipu")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Moves all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on IPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"load-state-dict"},(0,s.kt)("span",{className:"doc-symbol-name"},"load","_","state","_","dict")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, state_dict: Mapping[str, Any], strict: bool = True)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Copies parameters and buffers from :attr:`state_dict` into\nthis module and its descendants. If :attr:`strict` is ``True``, then\nthe keys of :attr:`state_dict` must exactly match the keys returned\nby this module's :meth:`~torch.nn.Module.state_dict` function.\n\nArgs:\nstate_dict (dict): a dict containing parameters and\npersistent buffers.\nstrict (bool, optional): whether to strictly enforce that the keys\nin :attr:`state_dict` match the keys returned by this module's\n:meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n\nReturns:\n``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n* **missing_keys** is a list of str containing the missing keys\n* **unexpected_keys** is a list of str containing the unexpected keys\n\nNote:\nIf a parameter or buffer is registered as ``None`` and its corresponding key\nexists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n``RuntimeError``.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"log-abs-det-jacobian"},(0,s.kt)("span",{className:"doc-symbol-name"},"log","_","abs","_","det","_","jacobian")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, x: 'torch.Tensor', y: 'torch.Tensor', context: 'Optional[torch.Tensor]' = None) -> 'torch.Tensor'")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nComputes the log det jacobian `log |dy/dx|` given input and output.\nBy default, assumes a volume preserving bijection.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"modules"},(0,s.kt)("span",{className:"doc-symbol-name"},"modules")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self) -> Iterator[ForwardRef('Module')]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over all modules in the network.\n\nYields:\nModule: a module in the network\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.modules()):\n...     print(idx, '->', m)\n\n0 -> Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -> Linear(in_features=2, out_features=2, bias=True)\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"named-buffers"},(0,s.kt)("span",{className:"doc-symbol-name"},"named","_","buffers")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over module buffers, yielding both the\nname of the buffer as well as the buffer itself.\n\nArgs:\nprefix (str): prefix to prepend to all buffer names.\nrecurse (bool): if True, then yields buffers of this module\nand all submodules. Otherwise, yields only buffers that\nare direct members of this module.\n\nYields:\n(str, torch.Tensor): Tuple containing the name and buffer\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, buf in self.named_buffers():\n>>>    if name in ['running_var']:\n>>>        print(buf.size())\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"named-children"},(0,s.kt)("span",{className:"doc-symbol-name"},"named","_","children")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self) -> Iterator[Tuple[str, ForwardRef('Module')]]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over immediate children modules, yielding both\nthe name of the module as well as the module itself.\n\nYields:\n(str, Module): Tuple containing a name and child module\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, module in model.named_children():\n>>>     if name in ['conv4', 'conv5']:\n>>>         print(module)\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"named-modules"},(0,s.kt)("span",{className:"doc-symbol-name"},"named","_","modules")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over all modules in the network, yielding\nboth the name of the module as well as the module itself.\n\nArgs:\nmemo: a memo to store the set of modules already added to the result\nprefix: a prefix that will be added to the name of the module\nremove_duplicate: whether to remove the duplicated module instances in the result\nor not\n\nYields:\n(str, Module): Tuple of name and module\n\nNote:\nDuplicate modules are returned only once. In the following\nexample, ``l`` will be returned only once.\n\nExample::\n\n>>> l = nn.Linear(2, 2)\n>>> net = nn.Sequential(l, l)\n>>> for idx, m in enumerate(net.named_modules()):\n...     print(idx, '->', m)\n\n0 -> ('', Sequential(\n(0): Linear(in_features=2, out_features=2, bias=True)\n(1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"named-parameters"},(0,s.kt)("span",{className:"doc-symbol-name"},"named","_","parameters")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over module parameters, yielding both the\nname of the parameter as well as the parameter itself.\n\nArgs:\nprefix (str): prefix to prepend to all parameter names.\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\n(str, Parameter): Tuple containing the name and parameter\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for name, param in self.named_parameters():\n>>>    if name in ['bias']:\n>>>        print(param.size())\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"param-shapes"},(0,s.kt)("span",{className:"doc-symbol-name"},"param","_","shapes")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, shape: torch.Size) -> Sequence[torch.Size]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"<empty docstring>\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"parameters"},(0,s.kt)("span",{className:"doc-symbol-name"},"parameters")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\nrecurse (bool): if True, then yields parameters of this module\nand all submodules. Otherwise, yields only parameters that\nare direct members of this module.\n\nYields:\nParameter: module parameter\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> for param in model.parameters():\n>>>     print(type(param), param.size())\n<class 'torch.Tensor'> (20L,)\n<class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-backward-hook"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","backward","_","hook")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Registers a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\nthe behavior of this function will change in future versions.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-buffer"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","buffer")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Adds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be\nconsidered a model parameter. For example, BatchNorm's ``running_mean``\nis not a parameter, but is part of the module's state. Buffers, by\ndefault, are persistent and will be saved alongside parameters. This\nbehavior can be changed by setting :attr:`persistent` to ``False``. The\nonly difference between a persistent buffer and a non-persistent buffer\nis that the latter will not be a part of this module's\n:attr:`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nArgs:\nname (str): name of the buffer. The buffer can be accessed\nfrom this module using the given name\ntensor (Tensor or None): buffer to be registered. If ``None``, then operations\nthat run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\nthe buffer is **not** included in the module's :attr:`state_dict`.\npersistent (bool): whether the buffer is part of this module's\n:attr:`state_dict`.\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> self.register_buffer('running_mean', torch.zeros(num_features))\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-forward-hook"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","forward","_","hook")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Registers a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has computed an output.\nIt should have the following signature::\n\nhook(module, input, output) -> None or modified output\n\nThe input contains only the positional arguments given to the module.\nKeyword arguments won't be passed to the hooks and only to the ``forward``.\nThe hook can modify the output. It can modify the input inplace but\nit will not have effect on forward since this is called after\n:func:`forward` is called.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-forward-pre-hook"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","forward","_","pre","_","hook")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Registers a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward` is invoked.\nIt should have the following signature::\n\nhook(module, input) -> None or modified input\n\nThe input contains only the positional arguments given to the module.\nKeyword arguments won't be passed to the hooks and only to the ``forward``.\nThe hook can modify the input. User can either return a tuple or a\nsingle modified value in the hook. We will wrap the value into a tuple\nif a single value is returned(unless that value is already a tuple).\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-full-backward-hook"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","full","_","backward","_","hook")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Registers a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to a module\nare computed, i.e. the hook will execute if and only if the gradients with\nrespect to module outputs are computed. The hook should have the following\nsignature::\n\nhook(module, grad_input, grad_output) -> tuple(Tensor) or None\n\nThe :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\nwith respect to the inputs and outputs respectively. The hook should\nnot modify its arguments, but it can optionally return a new gradient with\nrespect to the input that will be used in place of :attr:`grad_input` in\nsubsequent computations. :attr:`grad_input` will only correspond to the inputs given\nas positional arguments and all kwarg arguments are ignored. Entries\nin :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\narguments.\n\nFor technical reasons, when this hook is applied to a Module, its forward function will\nreceive a view of each Tensor passed to the Module. Similarly the caller will receive a view\nof each Tensor returned by the Module's forward function.\n\n.. warning ::\nModifying inputs or outputs inplace is not allowed when using backward hooks and\nwill raise an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-load-state-dict-post-hook"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","load","_","state","_","dict","_","post","_","hook")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, hook)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Registers a post hook to be run after module's ``load_state_dict``\nis called.\n\nIt should have the following signature::\nhook(module, incompatible_keys) -> None\n\nThe ``module`` argument is the current module that this hook is registered\non, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\nof attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\nis a ``list`` of ``str`` containing the missing keys and\n``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\nThe given incompatible_keys can be modified inplace if needed.\n\nNote that the checks performed when calling :func:`load_state_dict` with\n``strict=True`` are affected by modifications the hook makes to\n``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\nset of keys will result in an error being thrown when ``strict=True``, and\nclearning out both missing and unexpected keys will avoid an error.\n\nReturns:\n:class:`torch.utils.hooks.RemovableHandle`:\na handle that can be used to remove the added hook by calling\n``handle.remove()``\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-module"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","module")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Alias for :func:`add_module`.\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"register-parameter"},(0,s.kt)("span",{className:"doc-symbol-name"},"register","_","parameter")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Adds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArgs:\nname (str): name of the parameter. The parameter can be accessed\nfrom this module using the given name\nparam (Parameter or None): parameter to be added to the module. If\n``None``, then operations that run on parameters, such as :attr:`cuda`,\nare ignored. If ``None``, the parameter is **not** included in the\nmodule's :attr:`state_dict`.\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"requires-grad-"},(0,s.kt)("span",{className:"doc-symbol-name"},"requires","_","grad","_")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, requires_grad: bool = True) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Change if autograd should record operations on parameters in this\nmodule.\n\nThis method sets the parameters' :attr:`requires_grad` attributes\nin-place.\n\nThis method is helpful for freezing part of the module for finetuning\nor training parts of a model individually (e.g., GAN training).\n\nSee :ref:`locally-disable-grad-doc` for a comparison between\n`.requires_grad_()` and several similar mechanisms that may be confused with it.\n\nArgs:\nrequires_grad (bool): whether autograd should record operations on\nparameters in this module. Default: ``True``.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"set-extra-state"},(0,s.kt)("span",{className:"doc-symbol-name"},"set","_","extra","_","state")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, state: Any)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\nThis function is called from :func:`load_state_dict` to handle any extra state\nfound within the `state_dict`. Implement this function and a corresponding\n:func:`get_extra_state` for your module if you need to store extra state within its\n`state_dict`.\n\nArgs:\nstate (dict): Extra state from the `state_dict`\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"share-memory"},(0,s.kt)("span",{className:"doc-symbol-name"},"share","_","memory")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"See :meth:`torch.Tensor.share_memory_`\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"state-dict"},(0,s.kt)("span",{className:"doc-symbol-name"},"state","_","dict")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, *args, destination=None, prefix='', keep_vars=False)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Returns a dictionary containing references to the whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are\nincluded. Keys are corresponding parameter and buffer names.\nParameters and buffers set to ``None`` are not included.\n\n.. note::\nThe returned object is a shallow copy. It contains references\nto the module's parameters and buffers.\n\n.. warning::\nCurrently ``state_dict()`` also accepts positional arguments for\n``destination``, ``prefix`` and ``keep_vars`` in order. However,\nthis is being deprecated and keyword arguments will be enforced in\nfuture releases.\n\n.. warning::\nPlease avoid the use of argument ``destination`` as it is not\ndesigned for end-users.\n\nArgs:\ndestination (dict, optional): If provided, the state of module will\nbe updated into the dict and the same object is returned.\nOtherwise, an ``OrderedDict`` will be created and returned.\nDefault: ``None``.\nprefix (str, optional): a prefix added to parameter and buffer\nnames to compose the keys in state_dict. Default: ``''``.\nkeep_vars (bool, optional): by default the :class:`~torch.Tensor` s\nreturned in the state dict are detached from autograd. If it's\nset to ``True``, detaching will not be performed.\nDefault: ``False``.\n\nReturns:\ndict:\na dictionary containing a whole state of the module\n\nExample::\n\n>>> # xdoctest: +SKIP(\"undefined vars\")\n>>> module.state_dict().keys()\n['bias', 'weight']\n\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"to"},(0,s.kt)("span",{className:"doc-symbol-name"},"to")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, *args, **kwargs)")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'Moves and/or casts the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, dtype=None, non_blocking=False)\n:noindex:\n\n.. function:: to(dtype, non_blocking=False)\n:noindex:\n\n.. function:: to(tensor, non_blocking=False)\n:noindex:\n\n.. function:: to(memory_format=torch.channels_last)\n:noindex:\n\nIts signature is similar to :meth:`torch.Tensor.to`, but only accepts\nfloating point or complex :attr:`dtype`\\ s. In addition, this method will\nonly cast the floating point or complex parameters and buffers to :attr:`dtype`\n(if given). The integral parameters and buffers will be moved\n:attr:`device`, if that is given, but with dtypes unchanged. When\n:attr:`non_blocking` is set, it tries to convert/move asynchronously\nwith respect to the host if possible, e.g., moving CPU Tensors with\npinned memory to CUDA devices.\n\nSee below for examples.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndevice (:class:`torch.device`): the desired device of the parameters\nand buffers in this module\ndtype (:class:`torch.dtype`): the desired floating point or complex dtype of\nthe parameters and buffers in this module\ntensor (torch.Tensor): Tensor whose dtype and device are the desired\ndtype and device for all parameters and buffers in this module\nmemory_format (:class:`torch.memory_format`): the desired memory\nformat for 4D parameters and buffers in this module (keyword\nonly argument)\n\nReturns:\nModule: self\n\nExamples::\n\n>>> # xdoctest: +IGNORE_WANT("non-deterministic")\n>>> linear = nn.Linear(2, 2)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]])\n>>> linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n[-0.5113, -0.2325]], dtype=torch.float64)\n>>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n>>> gpu1 = torch.device("cuda:1")\n>>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16, device=\'cuda:1\')\n>>> cpu = torch.device("cpu")\n>>> linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n[-0.5112, -0.2324]], dtype=torch.float16)\n\n>>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n>>> linear.weight\nParameter containing:\ntensor([[ 0.3741+0.j,  0.2382+0.j],\n[ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n>>> linear(torch.ones(3, 2, dtype=torch.cdouble))\ntensor([[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j],\n[0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n\n\n')),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"to-empty"},(0,s.kt)("span",{className:"doc-symbol-name"},"to","_","empty")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, *, device: Union[str, torch.device]) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Moves the parameters and buffers to the specified device without copying storage.\n\nArgs:\ndevice (:class:`torch.device`): The desired device of the parameters\nand buffers in this module.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"train"},(0,s.kt)("span",{className:"doc-symbol-name"},"train")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, mode: bool = True) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Sets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of\nparticular modules for details of their behaviors in training/evaluation\nmode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\netc.\n\nArgs:\nmode (bool): whether to set training mode (``True``) or evaluation\nmode (``False``). Default: ``True``.\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"type"},(0,s.kt)("span",{className:"doc-symbol-name"},"type")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\nThis method modifies the module in-place.\n\nArgs:\ndst_type (type or string): the desired type\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"xpu"},(0,s.kt)("span",{className:"doc-symbol-name"},"xpu")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Moves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So\nit should be called before constructing optimizer if the module will\nlive on XPU while being optimized.\n\n.. note::\nThis method modifies the module in-place.\n\nArguments:\ndevice (int, optional): if specified, all parameters will be\ncopied to that device\n\nReturns:\nModule: self\n\n")),(0,s.kt)(d.Z,{mdxType:"PythonMethod"},(0,s.kt)("div",{className:"doc-method-row"},(0,s.kt)("div",{className:"doc-method-label"},(0,s.kt)("span",{className:"doc-symbol-label"},"member")),(0,s.kt)("div",{className:"doc-method-signature"},(0,s.kt)("h3",{id:"zero-grad"},(0,s.kt)("span",{className:"doc-symbol-name"},"zero","_","grad")),(0,s.kt)("span",{className:"doc-symbol-signature"},"(self, set_to_none: bool = False) -> None")))),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"Sets gradients of all model parameters to zero. See similar function\nunder :class:`torch.optim.Optimizer` for more context.\n\nArgs:\nset_to_none (bool): instead of setting to zero, set the grads to None.\nSee :meth:`torch.optim.Optimizer.zero_grad` for details.\n\n")))}b.isMDXComponent=!0}}]);